üìå Understanding Vectors in AI Systems
What Are Vectors?

Vectors are numeric representations of data and its related context. Each vector acts like a coordinate in a multi-dimensional space, where each dimension captures a specific feature or characteristic of the data.

For example, a food item could be represented by a vector where each dimension might represent a specific attribute like its spice level, sweetness, caloric content, etc.

üîÅ How Are Vectors Created?

Vectors are generated by sending raw data (like text, images, etc.) through an embedding model.

Pipeline:

Raw Data ‚Üí Encoder (Embedding Model) ‚Üí Vector Representation


These embeddings allow us to compare, cluster, and search through large volumes of data in an efficient and meaningful way.

üîç Similarity and Distance Functions

When comparing vectors, we rely on distance or similarity functions. These determine how "close" or "related" two pieces of data are in the vector space.

Common Distance/Similarity Functions:

Euclidean Distance: Measures the straight-line distance between the ends of two vectors.

Cosine Similarity: Measures the angle between two vectors, focusing on direction rather than magnitude.

Dot Product: Also angle-based, but includes magnitude, making it useful in certain scoring scenarios.

üß† Embedding Models vs LLMs

Embedding Models are specifically designed to convert data into vector form (numerical coordinates).

LLMs (Large Language Models) are designed to generate or understand language, but they can use embeddings for better understanding of context.

Note: Embedding models only produce vectors, not answers or full responses.

üß≠ Vector Search and Indexing

Vectors can be used to search for similar items using techniques like K-Nearest Neighbors (KNN). This is efficient when data is indexed.

Components:

Index Definition: Helps organize vector data for fast retrieval.

knnVector: Performs a nearest neighbor search to find similar points.

Dimensions: The embedding model defines the vector size (e.g., 1536 for text models). For image data, dimensions may differ.

HNSW (Hierarchical Navigable Small World): A commonly used algorithm for efficient vector search.

Create a Vector Search Index:

You can apply prefilters to narrow down search results before performing the vector search.

Aggregation pipelines can be used in real time to process vector search queries and retrieve results.

‚öôÔ∏è Benchmarking and Tools

There are several open-source tools available to benchmark the performance and accuracy of your vector search and embedding models.

üìö Retrieval-Augmented Generation (RAG)

RAG combines retrieval of relevant content with generation by LLMs.

Key Features:

The LLM doesn't rely solely on its training; it uses external knowledge bases to answer questions.

Enables light personalization based on retrieved context.

Limitations:

Struggles with complex or multi-step tasks

Cannot self-refine or revise its own outputs dynamically.

Example Use Case:
Travel assistant tools like Navan can use RAG for answering user queries about itineraries, bookings, and travel policies.

üöÄ Redis and LangCache Integration

Redis has integrated embedding models for caching purposes, particularly helpful in avoiding unnecessary token consumption in LLM calls.

LangCache is Redis's embedding-based caching layer.

Re-ranking methods are used to evaluate how similar a new question is to cached responses, improving performance and cost-efficiency.

üß† Components of an AI Agent

To behave intelligently, an AI agent requires the following components:

Perception:
Mechanism for understanding the environment. Inputs can include text, images, speech, etc.

Planning and Reasoning:
This is the "brain" of the agent. It plans actions or makes decisions. In LLMs, this is often done by prompting in a way that elicits the correct response.

Tools (Actions):
Interfaces the agent uses to interact with external systems (e.g., databases, APIs, file systems).

Memory:
Keeps track of context, history, or prior actions for long-term reasoning or personalization.

üß© Embedding Models and Providers

OyazAI Embedding Model: A custom or specialized embedding model.

AWS Bedrock Service: A fully managed service that offers access to foundation models from various providers, including embedding models for use in applications.

üß† Multimodal AI Components ‚Äì Cleaned & Clarified

1. LLaVA-01: Let Vision Language Models Ask

LLaVA stands for Large Language and Vision Assistant.

It‚Äôs a vision-language model (VLM) that enables multimodal capabilities ‚Äî allowing models to see and understand images alongside text.

LLaVA-01 is an early version combining a vision encoder (e.g., CLIP) with a language model (e.g., Vicuna) to answer questions about visual content.

2. Tesseract.exe in Multimodal Systems

Tesseract OCR (tesseract.exe) is an open-source optical character recognition tool.

Commonly used in multimodal AI systems to:

Extract text from images or scanned documents.

Convert visual text into machine-readable text before sending it to an LLM or embedding model.

Especially useful when combining image + text understanding.

3. OpenCLIP Library for Embeddings

OpenCLIP is an open-source implementation of OpenAI's CLIP model.

Used to generate embeddings (vector representations) from: Images , Text, (With extensions) Videos

These embeddings enable cross-modal search, similarity comparison, and retrieval between text and visuals.

4. Multimodal RAG with mongo.py

Refers to a Multimodal Retrieval-Augmented Generation (RAG) system that uses MongoDB as a backend for storing and retrieving vector data.

mongo.py is likely a custom script used to:

Insert and query vector embeddings (e.g., from OpenCLIP or LLaVA).

Perform vector similarity search using MongoDB Atlas Vector Search.

Combine both visual and textual data in RAG pipelines to improve LLM responses.

-----------------------------------------------------multimodel_mango,py--------------------------------------------------------------------------
# partition_pdf is used to filter heading, pagenumber, etc from actual data.

import os

import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.vectorstores import InMemoryVectorStore
from pymongo import MongoClient
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_ollama import OllamaEmbeddings
from langchain_ollama.llms import OllamaLLM
from langchain_text_splitters import RecursiveCharacterTextSplitter
from unstructured.partition.pdf import partition_pdf
from unstructured.partition.utils.constants import PartitionStrategy
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import unstructured_pytesseract.pytesseract as pt
pt.tesseract_cmd = r"your tesseract path"  # Windows tesseract
template = """
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
"""

pdfs_directory = 'pdfs/'
figures_directory = 'pdf_figures/'

mongo = MongoClient("mongodb://localhost:27017/?directConnection=true")
collection = mongo["rag"]["mongo_chunks"]

embeddings = HuggingFaceEmbeddings(model_name="mixedbread-ai/mxbai-embed-large-v1")
vector_store = MongoDBAtlasVectorSearch(
    embedding=embeddings,
    collection=collection, 
    index_name="pdf_media_idx", 
    text_key="text",
    embedding_key="embedding"
)

# vector_store = InMemoryVectorStore(embeddings)

model = OllamaLLM(model="gemma3")
vector_store.create_vector_search_index(
   dimensions = 1024 # The dimensions of the vector embeddings to be indexed
)
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)
custom_rag_prompt = PromptTemplate.from_template(template)
def format_docs(docs):
   return "\n\n".join(doc.page_content for doc in docs)
rag_chain = (
   {"context": retriever | format_docs, "question": RunnablePassthrough()}
   | custom_rag_prompt
   | model
   | StrOutputParser()
)


def upload_pdf(file):
    with open(pdfs_directory + file.name, "wb") as f:
        f.write(file.getbuffer())

def load_pdf(file_path):
    elements = partition_pdf(
        file_path,
        strategy=PartitionStrategy.HI_RES,
        extract_image_block_types=["Image", "Table"],
        extract_image_block_output_dir=figures_directory
    )

    text_elements = [element.text for element in elements if element.category not in ["Image", "Table"]]

    for file in os.listdir(figures_directory):
        extracted_text = extract_text(figures_directory + file)
        text_elements.append(extracted_text)

    return "\n\n".join(text_elements)

def extract_text(file_path):
    model_with_image_context = model.bind(images=[file_path])
    response = model_with_image_context.invoke("Tell me what do you see in this picture, describe in short 200 words only")
    print("Extracted text:", response)
    return response

def split_text(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_text(text)
    docs = [Document(page_content=chunk) for chunk in chunks]
    print(f"Split into {len(docs)} chunks.")
    print(docs[0])
    return docs

def index_docs(texts):
    vector_store.add_documents(texts)

def retrieve_docs(query):
    return retriever.invoke(query)

def answer_question(question):
    answer = rag_chain.invoke(question)
    return answer

uploaded_file = st.file_uploader(
    "Upload PDF",
    type="pdf",
    accept_multiple_files=False
)

if uploaded_file:
    upload_pdf(uploaded_file)
    text = load_pdf(pdfs_directory + uploaded_file.name)
    chunked_texts = split_text(text)
    index_docs(chunked_texts)

question = st.chat_input()

if question:
    st.chat_message("user").write(question)
    documents = retrieve_docs(question)
    print(f"Retrieved {documents} documents.")
    answer = answer_question(question)
    st.chat_message("assistant").write(answer)
